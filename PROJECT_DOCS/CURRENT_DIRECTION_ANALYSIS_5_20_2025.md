Fog-of-War Implementation in Mobile Apps: Techniques & Comparisons

Real-World GPS-Based Games and Apps

Pokémon GO and Geocaching – No Fog-of-War by Design

Niantic’s Pokémon GO and typical geocaching apps do not use a fog-of-war on their maps. The full base map is visible from the start – these apps focus on showing points of interest and game objects, not hiding terrain. For example, Pokémon GO’s map is fully exposed (no “fog”) as you pan/zoom around your area. This means they don’t need to track explored areas at all; there is no concept of “unexplored” territory on the map. As a result, memory management is simpler (no growing list of visited tiles), and there’s no risk of users “peeking” at hidden areas – because nothing is hidden to begin with. The trade-off is that these games don’t provide an exploration challenge via the map itself. They manage performance by only loading nearby game data (like Pokémon spawns or caches) rather than by obscuring the map. In short, Pokémon GO and geocaching apps bypass fog-of-war entirely, so their map rendering is straightforward (just standard map tiles) with no persistent reveal mechanics to implement.

Fog of World – Global Map Exploration with Permanent Reveal

Fog of World (2012) is a prime example of a real-world app that does implement a fog-of-war on a global scale ￼. In Fog of World, the entire world map starts shrouded, and traveling to new places uncovers those areas permanently for the user. All previously visited locations remain visible forever (once unfogged, they never revert to fog). To handle a potentially huge explored area, Fog of World uses a highly optimized approach: it stores your travel tracks and uncovered regions efficiently on-device (no user accounts or constant server sync) ￼. Under the hood, it likely stores visited regions as vector data (paths/polygons) rather than massive images, which allows it to render the fog overlay at high resolution and adapt it as you zoom. (The developers boast that Fog of World can show your travel “tracks” with seamlessly scaling thickness at any zoom level ￼, something that would be hard with naive bitmap overlays.) The app prevents “peeking” beyond the fog by overlaying an opaque layer on all unexplored areas – if you scroll to a place you’ve never been, you’ll just see fog (no map details) until you actually go there. In practice, Fog of World likely partitions the world into manageable chunks or uses map tiling to keep memory in check. Its custom database keeps storage minimal and performance high even as your explored area grows to entire cities or countries ￼. In summary, Fog of World demonstrates a successful real-world fog-of-war: it permanently records explored areas, covers everything else with a fog layer, and uses an efficient data model so that rendering remains smooth over years of exploration.

Other GPS Games (Orna, Ingress, etc.)

Other location-based games generally haven’t emphasized a fog-of-war mechanic for the base map. Ingress (Niantic’s predecessor to Pokémon GO) and RPGs like Orna show the map and nearby points, but they don’t hide the terrain behind fog. Typically, these games restrict interactions by distance (you can only perform actions nearby) but still let you see the map around you. That means they don’t need to remember “visited” areas – the map is always visible by default. One side effect is that there’s no notion of “clearing the fog” as motivation; instead, the gameplay incentives are placed on capturing locations or battling monsters, not on uncovering new map sections. From a technical perspective, this simplifies things: the app just uses the map provider’s tiles and maybe caches areas you’ve looked at recently, but it isn’t tracking a growing overlay of explored zones. So, while these games are GPS-based, their rendering challenges lie more in real-time object updates and network data, rather than maintaining a fog layer. This contrasts with an app like Fog of World, which specifically gamifies exploration by keeping unexplored areas obscured until you physically visit them.

Virtual Games (Strategy and Roguelike Examples)

RTS and Strategy Games – Tile Grids and Persistent Revelation

Classic strategy games (RTS and 4X) on PC and mobile use fog-of-war to hide portions of the game map until explored. Typically, the game world is a grid of tiles or a finite map. An array or grid data structure marks which tiles have been explored. For example, in Civilization or StarCraft, each map tile has a boolean flag for “discovered”; once the player uncovers a tile, it stays revealed (often rendered in a dimmed “discovered but not currently visible” state) ￼. This is straightforward to implement: the game initializes all tiles as unexplored (fogged). As units move, adjacent tiles flip to “explored” in the array, and the rendering engine stops drawing fog over those tiles permanently. Because these maps are limited in size, memory isn’t a big issue – a grid of even a few thousand by thousand cells is manageable. To prevent cheating or peeking, the game’s renderer always overlays unexplored tiles with an opaque covering. Even if you scroll or zoom the camera, unexplored areas remain blacked out. Many games optimize this by grouping fog tiles or using large textured overlays for big unexplored regions, but fundamentally it’s a tile-by-tile operation. One example is Populous (and other RTS classics), where once a piece of the map is discovered it “will remain visible for the rest of the game” ￼ – achieved simply by storing that state and ensuring the fog layer doesn’t redraw over it later. Overall, strategy games rely on this static grid approach: it’s memory-efficient and easy to update in real time as units move, but it’s tied to games with predefined map sizes (not continuous real-world maps).

Roguelike Dungeons – Grid Fog with Simple Masks

Many roguelike games (e.g. Pixel Dungeon on mobile) also use a fog-of-war for dungeon exploration. These are usually grid-based like strategy games, so they use a similar technique: a 2D array tracks explored cells. As you move through the dungeon, each cell you step into (and surrounding visibility range) gets marked as explored. The rendering engine then does not draw fog on those cells anymore, though it might still darken them if you move away (to indicate they’re explored but currently out of sight). In Pixel Dungeon’s open-source implementation, the fog-of-war is essentially treated as another tile map overlay – each tile either has fog or not, according to the explored boolean ￼. Because pure black/white edges can look ugly, Pixel Dungeon applies a bit of visual polish: it uses a marching squares algorithm to choose blended edge tiles for the fog, smoothing the transition between fog and visible area ￼ ￼. But under the hood, it’s still a simple binary grid. Performance and memory are trivial here (dungeon levels are small). Real-time updates are just setting array entries and redrawing the affected tiles, which even older mobile devices handle easily. There’s essentially no risk of “peeking” beyond the fog in such games – the engine never renders anything where the array says “unexplored.” One constraint, though, is that this approach doesn’t scale to huge continuous maps; it works great for self-contained levels or finite worlds. In summary, roguelikes use the most straightforward fog-of-war strategy: a grid-based mask that’s updated as the player moves, with all previously visited spots saved as explored so they remain visible thereafter.

Modern Mobile Games – Real-Time Fog Rendering via GPU

Some modern games (including mobile strategy titles) use more advanced rendering for fog-of-war, often leveraging the GPU for efficiency. Instead of a tile-by-tile overlay, they render the fog as a full-screen texture or shader that updates dynamically. For example, Iron Marines (a mobile RTS) implemented fog-of-war by drawing to a texture each frame: a secondary camera renders “vision blobs” (white shapes representing visible areas) onto a dark texture, which is then projected over the world as the fog layer ￼ ￼. In that system, unexplored areas are black (opaque), previously explored-but-not-currently-visible areas might be a dark gray (semi-transparent), and currently visible areas are fully clear. Importantly, they ensured previously visited locations remain visible in gray by not clearing the render texture between frames – effectively accumulating a permanent record of seen areas on that texture ￼. This is analogous to a painter’s algorithm: once a spot has been painted “seen” on the fog texture, they don’t paint it back to black. The result is a smooth, continuous fog-of-war that can handle arbitrary shapes (not just grid cells) and updates in real time as units move. The strength here is performance and visual quality: the heavy lifting is done by the GPU, so even on mobile it can run well with proper optimizations (as the Iron Marines devs note, their solution “performed pretty well on mobile devices” after tuning it ￼). It also easily prevents peeking — the fog texture covers the entire map layer, and only holes punched by vision are transparent. The downside is complexity: this approach requires setting up render textures, shaders, or OpenGL/DirectX calls, which is more involved than toggling tiles. It’s mostly used in games where a high-quality fog effect is desired and the development team has the engine support for it (Unity, Unreal, etc.). In summary, some cutting-edge games opt for a GPU-texture based fog-of-war, yielding smooth visuals and efficient updates, at the cost of a more complex architecture (and typically fixed-size game worlds so the texture can cover the whole area).

Fog-of-War Rendering Strategies: Strengths & Weaknesses

There are multiple ways to implement a fog-of-war or map-revealing mechanic. Below we compare several strategies and how they address key challenges (memory usage, performance, preventing “peeking,” permanent reveal, real-time updates). Each approach has pros and cons:

1. GPS “Fog Holes” as Coordinate Lists (Point/Radii arrays)

Approach: Store a list of coordinates (points) that the user has visited, each with an associated radius representing the cleared area around that point. In rendering, overlay fog everywhere except those circular “holes” at visited coordinates. This is the method currently used in Fog of Dog (the app’s state contains an array of exploredAreas, each with latitude, longitude, and a fixed radius) ￼.
• Pros: Very straightforward to implement. Data storage is light – just a list of points/radii (even 10,000 points is negligible in memory). Adding new visited areas in real time is easy (append a new point to the list) ￼. No complex image processing is needed; you can directly feed these points into map overlay components (e.g., drawing circles or polygon holes). It naturally ensures permanent visibility of visited spots by simply never removing points from the list. If implemented with a single large polygon and holes, it also inherently prevents peeking: all areas not in a “hole” remain covered by the polygon.
• Cons: The list can grow very large over long-term use (e.g., if a user travels widely for years, there could be tens of thousands of points). Redundant points can overlap (the current implementation already skips adding a new point if it’s very close to an existing one ￼, but overlap still occurs). Rendering many holes can become a performance bottleneck – for example, drawing hundreds or thousands of circles or polygon cutouts on a mobile map might lag, especially when zoomed out (all those holes have to be computed and drawn). There’s also a risk of visual artifacts if holes don’t seamlessly merge – overlapping circles might leave slivers of fog between them or overly bright spots where they overlap. Managing the fog layer at world-scale is tricky: if the user pans far beyond the area of all stored points, you need a strategy to cover those far reaches with fog (often done by using a large outer polygon that extends to the screen edges or beyond). In short, this approach can start simple but may require optimizations (like merging nearby points into bigger regions or culling distant holes) to remain efficient as data grows.

2. Raster/Bitmap Mask Overlay

Approach: Maintain a bitmap image representing the fog layer, where each pixel corresponds to a geographic area (for example, a large image or a set of tiled images covering the world or the play area). Initially, the bitmap is filled (fog everywhere). As the user moves, “erase” or make transparent the portions of the bitmap corresponding to visited areas. Then overlay this image on top of the map (with fog pixels drawn semi-transparent or opaque, and cleared pixels fully transparent to reveal the map beneath).
• Pros: A bitmap mask can naturally cover the entire world (or large regions) with one continuous image or a grid of images, so preventing peek-through is straightforward – if a pixel is still marked foggy, you won’t see the map there. It provides a visually unified fog layer (no seams between circles or shapes). Real-time updates are conceptually simple: every time there’s new GPS data, you calculate which pixels of the image should be cleared and update those. This approach ensures permanent memory of visited areas by storing them in the image: once a pixel is turned “clear,” you’d save that state (so the image can be cached or saved to disk, and reloaded with all past clear areas intact). Another advantage is that rendering can be efficient if done as a single overlay: the map just has to draw this image (with appropriate transparency) on top, which the GPU can handle easily – it doesn’t matter if 10 pixels or 10,000 pixels have been cleared, it’s just one texture.
• Cons: The challenge is memory and scalability. A single image large enough to cover even a city at high resolution could be huge (and for the whole world, impossible). Tiling helps (breaking the world into regions, each with its own bitmap mask), but then you have complexity in managing multiple images: loading the right tiles as the user moves, merging edges, etc. The resolution of the bitmap determines how precise the fog holes can be – too low resolution and the revealed areas will be blocky; too high and the images consume lots of memory and storage. Updating the bitmap in real time on React Native is non-trivial – there’s no built-in pixel drawing API in high-level RN. You might need to use a library or native module to manipulate images (e.g., using a canvas API or sending the data to native code for updating). This adds complexity and can be slow if done frequently (imagine updating a PNG on every GPS tick). Another issue is zooming: if the bitmap is at a fixed resolution, when you zoom the map, the fog holes might not scale nicely (they could appear to grow/shrink if not handled, or you’d need multiple resolution layers). Managing persistent storage is also an issue – you’d need to save the bitmap state periodically (which could be large). In summary, raster masks give a nice unified fog layer but come with major challenges in a global, zoomable context, particularly in a React Native app where direct pixel manipulation is not out-of-the-box.

3. Canvas or WebGL Dynamic Fog Layer

Approach: Use a real-time drawing canvas or GPU rendering context to draw the fog-of-war on the fly. This could be done with a HTML5 Canvas (via a WebView or a library like react-native-skia or react-native-canvas) or using WebGL/Metal through a library. The idea is to programmatically draw shapes for fog and clear areas each frame or on updates. For example, on each location update, draw a filled translucent layer covering the screen, then punch out circles (or other shapes) where the user has been. This drawn result becomes an overlay (or a mask via MaskedView) on the map.
• Pros: Highly flexible and real-time. You can get smooth shapes, gradients, and even animations on the fog layer using this method. It’s essentially how many game engines handle fog (render to an off-screen texture then overlay). Using a GPU-accelerated canvas or WebGL means updates can be efficient: drawing a circle or two per frame is trivial for a GPU. As in the Unity example, you can accumulate a persistent “visited” layer by not clearing the canvas fully – e.g., draw all past visited areas to a static canvas once, then each frame only draw the newly discovered part. This approach can easily support fancy visuals like blurring the fog edges ￼ or coloring the fog. It also naturally prevents any peeking: as long as your canvas covers the whole view and you draw fog everywhere except visited spots, the user can’t see through the fog. With a MaskedView (available in React Native), you can even invert the approach: use the drawn shapes as a mask so that the map is only visible where you’ve drawn holes (the RN app already experimented with this “mask” technique). The canvas/WebGL method scales better than a naive image because you’re not storing a giant bitmap of the whole world – you’re drawing what’s needed for the current view (you could redraw different regions as needed when the user moves).
• Cons: This approach demands more specialized implementation work and possibly native modules. In React Native, you’d likely need to incorporate something like React Native Skia or Expo GL. This introduces new dependencies and a learning curve if you’re not familiar with graphics programming. Also, synchronizing the drawing with the map’s coordinate system can be tricky: you have to convert GPS coordinates to canvas positions correctly as the user pans/zooms. (If the canvas is fixed to the screen, you must redraw the holes in the correct place whenever the map moves. If the canvas is part of the map view, you need to ensure it scales/moves in sync with map tiles.) Timing is an issue too – you might need to throttle drawing updates to the device’s refresh rate or handle multiple location updates in one frame. Another consideration is persistence: you’d have to store the visited coordinates anyway to redraw them when the app restarts or the user moves to a new area. Unlike the bitmap approach, you’re not automatically “recording” the fog state in an image – you’re recalculating it each time (which means you need an efficient way to know what to draw, e.g., quadtree or list of regions). Performance-wise, while GPUs can handle quite a lot, a very large number of shapes can still be a problem. For instance, drawing 5,000 circles onto a canvas might be slow on older devices; you’d need to consider drawing fewer, larger combined shapes if possible. In short, a canvas/WebGL overlay is powerful and likely the most performant option for complex fog-of-war on mobile (games use it!), but it requires significant architectural additions in a React Native app and careful handling of coordinate math and state.

4. Vector Tile Masking

Approach: Leverage map tiling and vector data to handle the fog. The world (or game map) is divided into tiles (e.g., using a standard slippy map tiling scheme or a quadtree). For each tile, maintain a polygon or mask that covers the unexplored parts of that tile. As the user explores, update or carve out those polygons. Essentially, the fog-of-war becomes an additional map layer composed of many small pieces (tiles), each of which can be shown/hidden as needed. This could be implemented by supplying a custom overlay to the map where each tile is rendered based on the user’s data (for example, a tile could be fully foggy if untouched, partially transparent if partly explored, or fully transparent if fully explored).
• Pros: Scalability and lazy loading. By tying fog data to tiles, you only load and render what’s in the current view (plus a margin). If a user has explored 100,000 small areas scattered worldwide, you don’t need to load all that at once – only the tiles covering the current map window are processed. This keeps memory and draw calls in check. Many mapping frameworks (like Mapbox GL) allow custom vector tile layers or overlays; you could integrate the fog as just another layer, benefiting from the map engine’s efficiency in drawing tiles. Updating fog in real time would involve determining which tile the user is in and updating that tile’s mask data (and possibly neighbors if the revealed radius crosses boundaries). Permanent storage of visited areas would be in a spatial index – effectively each tile has a record of visited sub-areas. This approach inherently prevents peeking: if a tile is not marked visited, it will render as fog. Even when zooming out to a world view, unexplored tiles would just all show as fog layers. Another advantage is that vector representations (like polygons for fog boundaries) can be quite compact and scale nicely at different zoom levels (no pixelation).
• Cons: Implementing this in a React Native context can be complex and potentially a heavy refactor. If you’re using something like Google Maps or Apple Maps via react-native-maps, there’s limited out-of-the-box support for custom tile layers with dynamic content. You might have to implement a custom URL tile provider or use an offline map library. This could involve generating tile images on the fly (which brings back some of the raster complexity) or using a library that supports vector tiles (which might mean switching to a different map SDK, like Mapbox’s). Maintaining consistency across zoom levels is tricky: you’d need rules for when a tile is considered “explored” (e.g., is it when any portion is visited or fully visited?). If partially, you’d have partial polygons which get subdivided at higher zooms. The data structure to store visited areas becomes more involved as well – essentially a spatial database keyed by tile. You might need to implement algorithms to split a visited circle across multiple tile squares. Another downside is the development speed: this approach is somewhat akin to writing a mini GIS system to track and render exploration. It’s powerful but likely overkill for most apps unless you already have a vector tile infrastructure. Debugging could also be harder, since issues might only appear at certain zooms or tile boundaries. In short, vector tile masking can be very efficient at scale, but in a React Native app it would demand significant architectural changes (possibly adopting a new map rendering library or building a lot of tiling logic yourself), which might not be justified for an app that can achieve its goals with simpler means.

Feasibility in React Native and Recommended Approaches

From the above strategies, some will integrate more easily into a React Native (Expo/TypeScript) codebase than others. It’s important to consider how much change each would require and how to maximize development speed without reinventing the wheel:
• Current Approach (Coordinate List + Polygon/Mask): This is already in place and working, so it’s the path of least resistance. To extend it without major refactors, focus on optimizations: for example, merging nearby fog holes so the array doesn’t balloon with overlapping points, or culling off-screen holes when rendering (if using a single large polygon with holes, you might already be covered; if drawing many separate shapes, consider only rendering those within the current viewport). Since the app moved to a MaskedView implementation, you can continue with that pattern – it’s essentially a canvas approach but using RN’s masking. One suggestion is to use a drawing library like React Native Skia to draw the mask shape (i.e. draw one big dark layer with transparent circles). The Skia canvas could render all holes each frame. This avoids the complexity of manual tile management and leverages the fact that Skia is optimized in C++ under the hood. It would require adding the Skia dependency, but that might be a smaller change than switching map frameworks. Given a backend developer’s perspective, think of it as adding a specialized module rather than changing the whole app architecture. Bottom line: The coordinate-list method can be improved incrementally – by optimizing the data (merge overlapping areas, limit array growth) and using a better rendering technique for the mask (Skia canvas or similar) – without scrapping the existing design.
• Raster/Bitmap Approach in RN: This would likely be the slowest path to implement from scratch. React Native doesn’t have a built-in efficient way to draw on bitmaps repeatedly. You’d have to integrate a native module or perhaps use something like Expo’s GL or Canvas API to manipulate images. It’s doable (for example, one could maintain an off-screen HTML5 canvas in a WebView to update a tile image), but this adds a lot of moving parts. The maintenance of images (saving/loading them as the user moves to new areas) could become a headache. If a quick result is the priority, this approach could bog you down in low-level details (byte arrays, file I/O, etc.). It may only be worth it if you find an existing library or example to piggy-back on. Otherwise, the development speed here is not great – it’s an “unnecessary refactor” in the sense that it doesn’t leverage RN’s strengths.
• Canvas/WebGL Approach in RN: This is promising if you’re willing to include a new library. React Native Skia, for instance, can render shapes extremely fast and can be tied into state updates. You’d essentially replicate what a game engine does but in RN: whenever the user moves, update a Skia drawing. The architecture change is moderate – you’d introduce a Skia canvas in your Map screen and use it as an overlay or mask. However, it won’t require rewriting your state management; you can feed it the same exploredAreas list (just have Skia draw circles from that list). The main challenge is syncing with map movements: you’ll need the current map region and zoom to scale/position the Skia drawings correctly. This is doable by listening to map camera changes (which you likely already handle for the polygon approach). Another alternative is using a WebGL approach via Expo’s GLView, but that’s lower-level than Skia. Since you’re already using Expo, Skia might integrate more smoothly. In terms of dev speed, expect some learning curve on coordinate transforms, but many have done custom overlays on maps, so there might be example code out there. Recommendation: If the polygon-with-holes method starts lagging and quick tweaks aren’t enough, adding a Skia-based mask could significantly boost performance without a full app rewrite.
• Vector Tile/Mapbox Approach: Adopting this would mean a larger rework. If you were to use Mapbox GL, for example, you could store your fog data as a GeoJSON and add it as a layer with styling (black fill, etc., with holes). Mapbox can handle thousands of points/polygons quite well using its internal rendering engine. But going from react-native-maps (which likely uses Apple/Google maps) to react-native-mapbox-gl or another library involves changes in how you load and display maps, manage tokens, etc. Additionally, implementing a tiling scheme for your data might require writing backend logic (e.g., generating vector tiles on a server or bundling a library to do it on-device). This is probably too heavy a refactor if your goal is to get improvements quickly. It’s a powerful solution if you were starting fresh or aiming to support massive datasets, but for an MVP-phase app it could introduce a lot of new complexity (and potential bugs). Unless you already have a need for Mapbox features, it might be faster to stick with the simpler overlay approach and optimize it.
• Preventing “Peeking” Without Complex Tile Systems: One concern raised is ensuring users can’t see beyond the fog by zooming or panning. With the current approaches, the key is to always have a fog overlay covering the entire screen (or more) at any time. If you use one big polygon with holes, make its outer polygon large enough to cover the whole view even at maximum zoom-out (you might define it as a rectangle spanning slightly more than the world’s bounds). If using a canvas/mask, ensure the mask view is the size of the map view and anchored, so when the map moves, new areas revealed on screen still have fog drawn until you explicitly clear them. Practically, this might mean when the map region changes, you check if the user has any visited points in the newly exposed area; if not, you might draw a temporary full-cover until data catches up. However, if you maintain a global list of visited coordinates and always draw fog for unexplored, “peeking” is automatically handled. A note for React Native: the MaskedView approach you have is conceptually sound – just make sure the mask component (with all the holes) updates whenever the map’s region or visited list updates. Testing by aggressively zooming/panning will reveal if any edges of the screen show unmaksed map (if so, expand your mask shapes accordingly).

Maximizing Development Speed: Given that you come from a Python/backend background, the goal should be to minimize low-level graphics work. The simplest path is to iterate on the solution you have:
• Continue using the Redux state of explored points (it works, just possibly refine how many points you store).
• Use high-level tools: e.g., if RN’s <Polygon> with holes is slow beyond a certain number of holes, consider drawing fewer but larger shapes. You could post-process your exploredAreas list: combine any that overlap significantly, or drop ones fully contained in others. This can reduce draw complexity without changing the fundamental approach.
• If needed, switch to a masking canvas but reuse your data and logic. For instance, instead of adding a new point and updating a <Polygon>, you update a Skia canvas drawing. The business logic (determining when to add a point, ensuring no duplicates near each other, etc.) remains the same.

Crucially, whichever approach you choose, preserve the permanent memory of visited locations. All of the strategies above can be made to keep old areas revealed – just don’t re-fog them. For example, if you went with a canvas, you’d maintain a off-screen buffer of already visited areas (like the “PreviousVisionTexture” in the Unity example ￼) and never redraw those as fog. In a simpler RN case, it could be as easy as storing the list of visited coordinates persistently and always redrawing all holes each time.

In conclusion, for a React Native app that’s already functioning with a fog-of-war, the recommended path is iterative enhancement rather than a ground-up rewrite. Storing GPS fog holes in an array is a valid approach – its weaknesses (memory and performance at scale) can be mitigated by merging areas and possibly using a more efficient rendering (canvas) for the overlay. More elaborate techniques like tile masking or heavy use of WebGL would solve the problems too, but at the cost of considerable development effort. Since speed is a priority, leveraging what’s already built and gradually refactoring (only where needed, e.g., swapping in a better rendering component) will yield the best results. This way, you maintain architectural familiarity while addressing the pain points, making the fog-of-war both robust and fast without a complete overhaul.

Sources: Real-world fog-of-war app and game examples ￼ ￼ ￼ ￼; Pixel Dungeon dev blog ￼; Fog of Dog code snippets ￼; Unity/strategy game fog-of-war techniques ￼ ￼ ￼.
